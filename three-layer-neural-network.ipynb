{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angie\\AppData\\Local\\Temp\\ipykernel_20976\\1505340667.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  return (1 / (1 + np.exp(-z)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 is done. Accuracy = 0.1203\n",
      "epoch: 2 is done. Accuracy = 0.1555\n",
      "epoch: 3 is done. Accuracy = 0.2123\n",
      "epoch: 4 is done. Accuracy = 0.3025\n",
      "epoch: 5 is done. Accuracy = 0.3687\n",
      "epoch: 6 is done. Accuracy = 0.4188\n",
      "epoch: 7 is done. Accuracy = 0.4918\n",
      "epoch: 8 is done. Accuracy = 0.5806\n",
      "epoch: 9 is done. Accuracy = 0.6498\n",
      "epoch: 10 is done. Accuracy = 0.7297\n",
      "epoch: 11 is done. Accuracy = 0.7998\n",
      "epoch: 12 is done. Accuracy = 0.8395\n",
      "epoch: 13 is done. Accuracy = 0.8646\n",
      "epoch: 14 is done. Accuracy = 0.8754\n",
      "epoch: 15 is done. Accuracy = 0.8929\n",
      "epoch: 16 is done. Accuracy = 0.8998\n",
      "epoch: 17 is done. Accuracy = 0.9026\n",
      "epoch: 18 is done. Accuracy = 0.905\n",
      "epoch: 19 is done. Accuracy = 0.9061\n",
      "epoch: 20 is done. Accuracy = 0.9056\n",
      "epoch: 21 is done. Accuracy = 0.9055\n",
      "epoch: 22 is done. Accuracy = 0.9051\n",
      "epoch: 23 is done. Accuracy = 0.9072\n",
      "epoch: 24 is done. Accuracy = 0.9087\n",
      "epoch: 25 is done. Accuracy = 0.9049\n",
      "epoch: 26 is done. Accuracy = 0.908\n",
      "epoch: 27 is done. Accuracy = 0.9086\n",
      "epoch: 28 is done. Accuracy = 0.9093\n",
      "epoch: 29 is done. Accuracy = 0.9035\n",
      "epoch: 30 is done. Accuracy = 0.911\n",
      "\n",
      "Classification accuracy: \n",
      "0.911\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# read the MNIST data\n",
    "testx = np.loadtxt(\"testx.csv\", delimiter=',', dtype=int)\n",
    "testy = np.loadtxt(\"testy.csv\", delimiter=',', dtype=int)\n",
    "trainx = np.loadtxt(\"trainx.csv\", delimiter=',', dtype=int)\n",
    "trainy = np.loadtxt(\"trainy.csv\", delimiter=',', dtype=int)\n",
    "\n",
    "L = 3                 # number of layers including input and output\n",
    "sizes = [784, 30, 10] # number of neurons in each layer\n",
    "\n",
    "# the sigmoid activation function\n",
    "def f(z):\n",
    "    return (1 / (1 + np.exp(-z)))\n",
    "\n",
    "# sigmoid derivagtive function from sigmoid quiz\n",
    "def fprime(z):\n",
    "    return f(z) * (1 - f(z))\n",
    "\n",
    "# convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "def digit2vector(d):\n",
    "    return np.concatenate((np.repeat(0, d), [1], np.repeat(0, 9-d)))\n",
    "\n",
    "# a feedforward function that returns the activations\n",
    "# from each layer and the weighted inputs to each layer\n",
    "# so that they can be used during backpropagation.\n",
    "# W,b contain the weights, biases in the network.\n",
    "# x is the input of a single training example (a vector of length 784).\n",
    "def feedforward(W, b, x):\n",
    "    a = [x, np.zeros(sizes[1]), np.zeros(sizes[2])] # x is input layer\n",
    "    z = [x, np.zeros(sizes[1]), np.zeros(sizes[2])]\n",
    "\n",
    "    z[1] = np.dot(W[0], x) + b[0] #weighted input layer 1\n",
    "    a[1] = f(z[1]) #activation layer 1 using sigmoid\n",
    "    z[2] = np.dot(W[1] , a[1]) + b[1] #weighted input layer 2\n",
    "    a[2] = f(z[2]) #activation layer 2 using sigmoid\n",
    "    return a, z \n",
    "\n",
    "# given an input vector, return the predicted digit\n",
    "def classify(W, b, x):\n",
    "    a,z = feedforward(W,b,x)\n",
    "    h = a[2] # h is the column of values mapped from sigmoid function\n",
    "    return np.argmax(h) # the index of the largest value is the number classification\n",
    "\n",
    "\n",
    "# helper function for backprop().\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "def compute_error(W, a, z, y):\n",
    "    δ = [np.zeros(sizes[0]), np.zeros(sizes[1]), np.zeros(sizes[2])]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[2] = -(digit2vector(y) - a[2]) * fprime(z[2])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[1] = np.dot(W[1].T, δ[2]) * fprime(z[1])\n",
    "\n",
    "    return δ\n",
    "\n",
    "\n",
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements the equations BP3 and BP4.\n",
    "def compute_gradients(δ, a):\n",
    "    grad_W = [np.zeros((sizes[1], sizes[0])), np.zeros((sizes[2], sizes[1]))]\n",
    "    grad_b = [np.zeros(sizes[1]), np.zeros(sizes[2])]\n",
    "    #grad_b is δ[1],δ[2] from derivative proof\n",
    "    grad_b[0] = δ[1]\n",
    "    grad_b[1] = δ[2]\n",
    "    #grad_W is δ[1]*a[0],δ[2]*a[1] from question 2 of homework proof\n",
    "    grad_W[0] = np.outer(δ[1] , a[0]) #np.outer ensure that this results in a 30x784 matrix\n",
    "    grad_W[1] = np.outer(δ[2] , a[1]) #results in a 10x30 matrix\n",
    "    return grad_W, grad_b\n",
    "\n",
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "def backprop(W, b, x, y):\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (grad_W, grad_b) = compute_gradients(δ, a)\n",
    "    return grad_W, grad_b\n",
    "\n",
    "\n",
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "def GD(W, b, batch, α=0.01, λ=0.01):\n",
    "    m = len(batch)    # batch size\n",
    "\n",
    "    # data structure to accumulate the sum over the batch.\n",
    "    # in the lecture notes as well as Ng's article, sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ np.zeros((sizes[1], sizes[0])),\n",
    "             np.zeros((sizes[2], sizes[1])) ]\n",
    "    sumb = [ np.zeros(sizes[1]), np.zeros(sizes[2])]\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "    for i in range(m):\n",
    "        grad_W, grad_b = backprop(W,b,trainx[batch[i],:], trainy[batch[i]]) #index of training data x and y is from batch indicies, first index from batch used to select the row from trainx\n",
    "        sumW[0] += grad_W[0] #each index of gradient is added to the sum\n",
    "        sumW[1] += grad_W[1]\n",
    "        sumb[0] += grad_b[0]\n",
    "        sumb[1] += grad_b[1]\n",
    "    \n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "\n",
    "    #each index of W and b is updated with the step and average gradient (and regularization term for weights)\n",
    "    W[0] = W[0] - α * (((1/m)*sumW[0]) + (λ * W[0]))\n",
    "    W[1] = W[1] - α * (((1/m)*sumW[1]) + (λ * W[1]))\n",
    "    b[0] = b[0] - α * ((1/m) * sumb[0])\n",
    "    b[1] = b[1] - α * ((1/m) * sumb[1])\n",
    "\n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "\n",
    "    return W, b, grad_W, grad_b\n",
    "\n",
    "\n",
    "# classify the test data and compute the classification accuracy\n",
    "def accuracy(W, b): \n",
    "    ntest = len(testy)\n",
    "    yhat = np.zeros(ntest)\n",
    "    for i in range(ntest):\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    hit_rate =  np.sum(testy == yhat) / ntest # hit rate\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "# train the neural network using batch gradient descent.\n",
    "# this is a driver function to repeatedly call GD().\n",
    "# N = number of observations in the training data.\n",
    "# m = batch size\n",
    "# α = learning rate (i.e. step size)\n",
    "# λ = regularization parameter\n",
    "def BGD(N, m, epochs, α=0.01, λ=0.01):\n",
    "    sizes = [784, 30, 10]\n",
    "    #random initialization of the weights and biases\n",
    "    #python uses np.random.normal to directly generate random numbers from a normal distribution\n",
    "    W = [ np.random.normal(0,1, (sizes[1], sizes[0])),  # layer 1 to 2\n",
    "          np.random.normal(0,1, (sizes[2], sizes[1])) ] # layer 2 to 3\n",
    "    b = [ np.random.normal(0,1, sizes[1]),   # layer 2\n",
    "          np.random.normal(0,1, sizes[2]) ]  # layer 3\n",
    "    \n",
    "    grad_W = [ np.zeros((sizes[1], sizes[0])),  # layer 1 to 2\n",
    "          np.zeros((sizes[2], sizes[1])) ] # layer 2 to 3\n",
    "    grad_b = [ np.zeros(sizes[1]),   # layer 2\n",
    "          np.zeros(sizes[2]) ]   # layer 3\n",
    "\n",
    "    \n",
    "    for i in range(epochs): \n",
    "        remaining = list(range(N)) #convert to list of 0:N-1 to track indicies\n",
    "        while len(remaining) > 0: # while there are still observations to create batches from \n",
    "            batch = random.sample(remaining, m) #samples batch of m from the remaining indicies\n",
    "            for index in batch:\n",
    "                remaining.remove(index) #removes the batches indicies from remaining\n",
    "            W,b,grad_W,grad_b = GD(W,b,batch)\n",
    "        print(f\"epoch: {i + 1} is done. Accuracy = {accuracy(W,b)}\")\n",
    "\n",
    "    return W, b, grad_W, grad_b\n",
    "\n",
    "\n",
    "# some tuning parameters\n",
    "N = len(trainy)\n",
    "m = 25       # batch size\n",
    "epochs = 30  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01     # regularization parameter\n",
    "W, b, grad_W, grad_b = BGD(N, m, epochs, α=α, λ=λ)\n",
    "print()\n",
    "print(\"Classification accuracy: \")\n",
    "print(accuracy(W,b))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
